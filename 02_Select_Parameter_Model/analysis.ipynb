{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Optimal Parameter\n",
    "## Introduction\n",
    "\n",
    "In this notebook, there are several sections that describe the function.\n",
    "1. OOB_ParamGridSearch function -> gridsearch_model.py\n",
    "    - Full function of OOB_ParamGridSearch\n",
    "    - Decomposing code for testing\n",
    "      + fit\n",
    "      + fit_score\n",
    "      + oob_score_accuracy\n",
    "2. Evaluation function -> eval_metrics.py  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Full function of OOB_ParamGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import eval_metrics\n",
    "\n",
    "class OOB_ParamGridSearch:\n",
    "    def __init__(self, \n",
    "                 estimator, \n",
    "                 param_grid,\n",
    "                 seed,\n",
    "                 n_jobs=-1, \n",
    "                 refit=True, \n",
    "                 task=\"regression\", \n",
    "                 metric=\"mse\"):\n",
    "        \"\"\"\n",
    "        Initializes the OOB_ParamGridSearch class.\n",
    "\n",
    "       \n",
    "        :param estimator (object): The base estimator to be used.\n",
    "        :param param_grid (dict or list of dicts): The parameter grid to search over.\n",
    "        :param seed (int): The random \n",
    "        :param n_jobs (int, optional): The number of jobs to run in parallel. Defaults to -1.\n",
    "        :param refit (bool, optional): Indicates whether to refit the model with the best hyperparameters. Defaults to True.\n",
    "        :param task (str, optional): The task type, either \"classification\" or \"regression\". Defaults to \"classification\".\n",
    "        :param metric (str, optional): The evaluation metric to use. Defaults to \"mse\".\n",
    "        \"\"\"\n",
    "        self.n_jobs = n_jobs\n",
    "        self.seed = seed \n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.refit = refit\n",
    "        self.task = task\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            y_train):\n",
    "        \"\"\"\n",
    "        Fits the model with the given training data using the parameter grid search.\n",
    "\n",
    "        :param X_train (array-like): The input features for training.\n",
    "        :param y_train (array-like): The target values for training.\n",
    "\n",
    "        :return self (object): Returns self.\n",
    "        \"\"\"\n",
    "        params_iterable = list(ParameterGrid(self.param_grid))\n",
    "        parallel = joblib.Parallel(self.n_jobs)\n",
    "\n",
    "        output = parallel(\n",
    "            joblib.delayed(self.fit_and_score)(deepcopy(self.estimator), X_train, y_train, parameters)\n",
    "            for parameters in params_iterable)\n",
    "\n",
    "        output_array = np.array(output, dtype=np.float64)\n",
    "\n",
    "        best_index = np.argmin(output_array)\n",
    "        self.best_score_ = output_array[best_index]\n",
    "        self.best_param_ = params_iterable[best_index]\n",
    "\n",
    "        cv_results = pd.DataFrame(output, columns=['OOB_Error_Score'])\n",
    "        df_params = pd.DataFrame(params_iterable)\n",
    "        cv_results = pd.concat([cv_results, df_params], axis=1)\n",
    "        cv_results[\"params\"] = params_iterable\n",
    "        self.cv_results = (cv_results.\n",
    "                           sort_values(['OOB_Error_Score'], ascending=True).\n",
    "                           reset_index(drop=True))\n",
    "\n",
    "        if self.refit:\n",
    "            # Final fit with best hyperparameters\n",
    "            self.cv_model = deepcopy(self.estimator)(rseed=1, **self.best_param_)\n",
    "            self.cv_model.fit(X_train, y_train, feature_weight=None)\n",
    "            self.cv_model.save_model(\"/exeh_4/yuping/123.pkl\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_and_score(self, \n",
    "                      estimator, \n",
    "                      X_train, \n",
    "                      y_train, \n",
    "                      parameters):\n",
    "        \"\"\"\n",
    "        Fits the model and calculates the out-of-bag (OOB) error score.\n",
    "\n",
    "        :param estimator (object): The estimator object.\n",
    "        :param X_train (array-like): The input features for training.\n",
    "        :param y_train (array-like): The target values for training.\n",
    "        :param parameters (dict): The hyperparameters to use for fitting the model.\n",
    "\n",
    "        :return oob_error (float): The calculated out-of-bag error score.\n",
    "        \"\"\"\n",
    "        train_model = estimator(rseed=self.seed, **parameters)\n",
    "        train_model.fit(X_train, y_train, feature_weight=None)\n",
    "        oob_error = 1 - self.oob_score_accuracy(train_model, X_train, y_train, task=self.task, metric=self.metric)\n",
    "\n",
    "        return oob_error\n",
    "\n",
    "    def oob_score_accuracy(self, \n",
    "                           rf, \n",
    "                           X_train, \n",
    "                           y_train, \n",
    "                           task, \n",
    "                           metric):\n",
    "        \"\"\"\n",
    "        Calculates the out-of-bag (OOB) score accuracy.\n",
    "\n",
    "       \n",
    "        :param rf (object): The random forest model.\n",
    "        :param X_train (array-like): The input features for training.\n",
    "        :param y_train (array-like): The target values for training.\n",
    "        :param task (str): The task type, either \"classification\" or \"regression\".\n",
    "        :param metric (str): The evaluation metric to use.\n",
    "\n",
    "        :return oob_score (float): The calculated out-of-bag score accuracy.\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble._forest import _generate_unsampled_indices, _get_n_samples_bootstrap\n",
    "\n",
    "        X = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "        y = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "        if task == \"classification\":\n",
    "            n_samples = len(X)\n",
    "            n_classes = len(np.unique(y))\n",
    "            predictions = np.zeros((n_samples, n_classes))\n",
    "            for tree in getattr(rf, \"model\").estimators_:\n",
    "                n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "                unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "\n",
    "                tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "                predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "            oob_score = eval_metrics.get_evaluation_report(predictions, y, task, metric)\n",
    "\n",
    "            return oob_score\n",
    "\n",
    "        else:\n",
    "            n_samples = len(X)\n",
    "            predictions = np.zeros(n_samples)\n",
    "            n_predictions = np.zeros(n_samples)\n",
    "            for tree in getattr(rf, \"model\").estimators_:\n",
    "                n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "                unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "\n",
    "                tree_preds = tree.predict(X[unsampled_indices, :])\n",
    "                predictions[unsampled_indices] += tree_preds\n",
    "                n_predictions[unsampled_indices] += 1\n",
    "\n",
    "            predictions /= n_predictions\n",
    "\n",
    "            oob_score = eval_metrics.get_evaluation_report(predictions, y, task, metric)\n",
    "\n",
    "            return oob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.OOB_ParamGridSearch at 0x7fe42d286790>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import model as im\n",
    "\n",
    "X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 100],\n",
    "    'max_depth': [2, 3]\n",
    "}\n",
    "\n",
    "oob_gridsearch = OOB_ParamGridSearch(n_jobs=1,\n",
    "                                     estimator=im.IterativeRFRegression,\n",
    "                                     param_grid=param_grid,\n",
    "                                     seed=123,\n",
    "                                     refit=True,\n",
    "                                     task=\"regression\",\n",
    "                                     metric=\"mse\")\n",
    "\n",
    "\n",
    "oob_gridsearch.fit(X_train=X, y_train=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing code for testing\n",
    "+ fit\n",
    "+ fit_score\n",
    "+ oob_score_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OOB_Error_Score  max_depth  n_estimators\n",
      "0         0.033333          3           100\n",
      "1         0.040000          3            30\n",
      "2         0.046667          3            20\n",
      "3         0.053333          2            30\n",
      "4         0.060000          2            20\n",
      "5         0.060000          2           100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import datasets\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import model as im\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# Separate out the data\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 100],\n",
    "    'max_depth': [2, 3]\n",
    "}\n",
    "\n",
    "def fit(X, y, param_grid):\n",
    "\n",
    "    params_iterable = list(ParameterGrid(param_grid))\n",
    "\n",
    "    parallel = joblib.Parallel(n_jobs=1)\n",
    "\n",
    "    output = parallel(\n",
    "              joblib.delayed(_fit_and_score)(deepcopy(\n",
    "                im.IterativeRFClassifier), X, y,parameters)\n",
    "            for parameters in params_iterable)\n",
    "\n",
    "\n",
    "    n_candidates = len(params_iterable)\n",
    "    a=np.array(output, dtype=np.float64)\n",
    "\n",
    "    best_index = np.argmin(a)\n",
    "    best_score_ = a[best_index]\n",
    "    best_param_ = params_iterable[best_index]\n",
    "\n",
    "    cv_results = pd.DataFrame(output, columns=['OOB_Error_Score'])\n",
    "    df_params = pd.DataFrame(params_iterable)\n",
    "    cv_results = pd.concat([cv_results, df_params], axis = 1)\n",
    "\n",
    "\n",
    "    cv_results = (cv_results.\n",
    "                  sort_values(['OOB_Error_Score'],ascending=True).\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "def _fit_and_score(estimator, X, y, parameters):\n",
    "\n",
    "\n",
    "    train_model = estimator(rseed=1, **parameters)\n",
    "    train_model.fit(X, y,  feature_weight=None)\n",
    "    oob_error = 1 - oob_score_accuracy(train_model, X, y)\n",
    "\n",
    "    return oob_error\n",
    "\n",
    "\n",
    "def oob_score_accuracy(rf, X, y):\n",
    "    from sklearn.ensemble._forest import _generate_unsampled_indices, _get_n_samples_bootstrap\n",
    "\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_classes = len(np.unique(y))\n",
    "    predictions = np.zeros((n_samples, n_classes))\n",
    "    for tree in getattr(rf, \"model\").estimators_:\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "        unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "\n",
    "        tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "        predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "    predicted_class_indexs = np.argmax(predictions, axis=1)\n",
    "    predicted_class = [getattr(rf, \"model\").classes_[i] for i in predicted_class_indexs]\n",
    "    \n",
    "    oob_score = np.mean(y == predicted_class)\n",
    "    \n",
    "    return oob_score\n",
    "\n",
    "oob_gridsearch = fit(X, y, param_grid)\n",
    "print(oob_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaulation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.4546273335677"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(X, y)\n",
    "prediction = regr.predict(X)\n",
    "\n",
    "def get_evaluation_report(y_pred, y_true, task, metric):\n",
    "    \"\"\"\n",
    "    Get values for common evaluation metrics\n",
    "\n",
    "    :param y_pred: predicted values\n",
    "    :param y_true: true values\n",
    "    :param task: ML task to solve\n",
    "    :param metic: choose specificed metric to assess the performance\n",
    "\n",
    "    :return: dictionary with specificed metrics\n",
    "    \"\"\"\n",
    "   \n",
    "    if task == 'classification':\n",
    "        average = 'micro' if len(np.unique(y_true)) > 2 else 'binary'\n",
    "        eval_report_dict = {\n",
    "            'auroc': sklearn.metrics.roc_auc_score(y_true=y_true, y_pred=y_pred, average=average),\n",
    "            'aupr': sklearn.metrics.average_precision_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "        }\n",
    "        eval_report_dict = eval_report_dict[metric]\n",
    "    else:\n",
    "        eval_report_dict = {\n",
    "            'mse': sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred),\n",
    "            'rmse': sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False),\n",
    "            'r2_score': sklearn.metrics.r2_score(y_true=y_true, y_pred=y_pred),\n",
    "        }\n",
    "        eval_report_dict = eval_report_dict[metric]\n",
    "        \n",
    "    return eval_report_dict\n",
    "\n",
    "\n",
    "get_evaluation_report(prediction, y, task=\"regression\",metric=\"mse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp_r",
   "language": "python",
   "name": "yp_r"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
