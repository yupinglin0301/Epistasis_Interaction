{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Optimal Parameter\n",
    "## Introduction\n",
    "\n",
    "In this notebook, there are several sections that describe the function.\n",
    "- OOB_ParamGridSearch function (gridsearch_model.py)\n",
    "    - Full function of OOB_ParamGridSearch\n",
    "    - Decomposing code for testing\n",
    "      + fit\n",
    "      + fit_score\n",
    "      + oob_score_accuracy\n",
    "- Evaluation function (eval_metrics.py) \n",
    "- RF_OOB_Dataset function (dataset_model.py)\n",
    "    - from_config\n",
    "    - get_samples\n",
    "    - get_features\n",
    "    - shuffle_data\n",
    "    - train_test_split\n",
    "    - generate_labels\n",
    "    - save_pipeline\n",
    "    - load_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  OOB_ParamGridSearch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import eval_metrics\n",
    "\n",
    "class OOB_ParamGridSearch:\n",
    "    def __init__(self, \n",
    "                 estimator, \n",
    "                 param_grid,\n",
    "                 seed,\n",
    "                 n_jobs=-1, \n",
    "                 refit=True, \n",
    "                 task=\"regression\", \n",
    "                 metric=\"mse\"):\n",
    "        \"\"\"\n",
    "        Initializes the OOB_ParamGridSearch class.\n",
    "\n",
    "       \n",
    "        :param estimator (object): The base estimator to be used.\n",
    "        :param param_grid (dict or list of dicts): The parameter grid to search over.\n",
    "        :param seed (int): The random for reproducibility\n",
    "        :param n_jobs (int, optional): The number of jobs to run in parallel. Defaults to -1.\n",
    "        :param refit (bool, optional): Indicates whether to refit the model with the best hyperparameters. Defaults to True.\n",
    "        :param task (str, optional): The task type, either \"classification\" or \"regression\". Defaults to \"classification\".\n",
    "        :param metric (str, optional): The evaluation metric to use. Defaults to \"mse\".\n",
    "        \"\"\"\n",
    "        self.n_jobs = n_jobs\n",
    "        self.seed = seed \n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.refit = refit\n",
    "        self.task = task\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            y_train):\n",
    "        \"\"\"\n",
    "        Fits the model with the given training data using the parameter grid search.\n",
    "\n",
    "        :param X_train (array-like): The input features for training.\n",
    "        :param y_train (array-like): The target values for training.\n",
    "\n",
    "        :return self (object): Returns self.\n",
    "        \"\"\"\n",
    "        params_iterable = list(ParameterGrid(self.param_grid))\n",
    "        parallel = joblib.Parallel(self.n_jobs)\n",
    "\n",
    "        output = parallel(\n",
    "            joblib.delayed(self.fit_and_score)(deepcopy(self.estimator), X_train, y_train, parameters)\n",
    "            for parameters in params_iterable)\n",
    "\n",
    "        output_array = np.array(output)\n",
    "\n",
    "        best_index = np.argmin(output_array)\n",
    "        self.best_score_ = output_array[best_index]\n",
    "        self.best_param_ = params_iterable[best_index]\n",
    "\n",
    "        cv_results = pd.DataFrame(output, columns=['OOB_Error_Score'])\n",
    "        df_params = pd.DataFrame(params_iterable)\n",
    "        cv_results = pd.concat([cv_results, df_params], axis=1)\n",
    "        cv_results[\"params\"] = params_iterable\n",
    "        self.cv_results = (cv_results.\n",
    "                           sort_values(['OOB_Error_Score'], ascending=True).\n",
    "                           reset_index(drop=True))\n",
    "\n",
    "        if self.refit:\n",
    "            # Final fit with best hyperparameters\n",
    "            cv_model = deepcopy(self.estimator)(rseed=self.seed, **self.best_param_)\n",
    "            cv_model.fit(X_train, y_train, feature_weight=None)\n",
    "            self.cv_model = cv_model\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit_and_score(self, \n",
    "                      estimator, \n",
    "                      X_train, \n",
    "                      y_train, \n",
    "                      parameters):\n",
    "        \"\"\"\n",
    "        Fits the model and calculates the out-of-bag (OOB) error score.\n",
    "\n",
    "        :param estimator (object): The estimator object.\n",
    "        :param X_train (array-like): The input features for training.\n",
    "        :param y_train (array-like): The target values for training.\n",
    "        :param parameters (dict): The hyperparameters to use for fitting the model.\n",
    "\n",
    "        :return oob_error (float): The calculated out-of-bag error score.\n",
    "        \"\"\"\n",
    "        train_model = estimator(rseed=self.seed, **parameters)\n",
    "        train_model.fit(X_train, y_train, feature_weight=None)\n",
    "        oob_error = 1 - self.oob_score_accuracy(train_model, X_train, y_train, task=self.task, metric=self.metric)\n",
    "\n",
    "        return oob_error\n",
    "\n",
    "    def oob_score_accuracy(self, \n",
    "                           rf, \n",
    "                           X_train, \n",
    "                           y_train, \n",
    "                           task, \n",
    "                           metric):\n",
    "        \"\"\"\n",
    "        Calculates the out-of-bag (OOB) score accuracy.\n",
    "\n",
    "       \n",
    "        :param rf (object): The random forest model.\n",
    "        :param X_train (array-like): The input features for training.\n",
    "        :param y_train (array-like): The target values for training.\n",
    "        :param task (str): The task type, either \"classification\" or \"regression\".\n",
    "        :param metric (str): The evaluation metric to use.\n",
    "\n",
    "        :return oob_score (float): The calculated out-of-bag score accuracy.\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble._forest import _generate_unsampled_indices, _get_n_samples_bootstrap\n",
    "\n",
    "        X = X_train.values if isinstance(X_train, pd.DataFrame) else X_train\n",
    "        y = y_train.values if isinstance(y_train, pd.Series) else y_train\n",
    "\n",
    "        if task == \"classification\":\n",
    "            n_samples = len(X)\n",
    "            n_classes = len(np.unique(y))\n",
    "            predictions = np.zeros((n_samples, n_classes))\n",
    "            for tree in getattr(rf, \"model\").estimators_:\n",
    "                n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "                unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "\n",
    "                tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "                predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "            oob_score = eval_metrics.get_evaluation_report(predictions, y, task, metric)\n",
    "\n",
    "            return oob_score\n",
    "\n",
    "        else:\n",
    "            n_samples = len(X)\n",
    "            predictions = np.zeros(n_samples)\n",
    "            n_predictions = np.zeros(n_samples)\n",
    "            for tree in getattr(rf, \"model\").estimators_:\n",
    "                n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "                unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "\n",
    "                tree_preds = tree.predict(X[unsampled_indices, :])\n",
    "                predictions[unsampled_indices] += tree_preds\n",
    "                n_predictions[unsampled_indices] += 1\n",
    "\n",
    "            predictions /= n_predictions\n",
    "\n",
    "            oob_score = eval_metrics.get_evaluation_report(predictions, y, task, metric)\n",
    "\n",
    "            return oob_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798  2.2408932 ]\n",
      " [ 1.86755799 -0.97727788  0.95008842 -0.15135721]\n",
      " [-0.10321885  0.4105985   0.14404357  1.45427351]\n",
      " [ 0.76103773  0.12167502  0.44386323  0.33367433]\n",
      " [ 1.49407907 -0.20515826  0.3130677  -0.85409574]\n",
      " [-2.55298982  0.6536186   0.8644362  -0.74216502]\n",
      " [ 2.26975462 -1.45436567  0.04575852 -0.18718385]\n",
      " [ 1.53277921  1.46935877  0.15494743  0.37816252]\n",
      " [-0.88778575 -1.98079647 -0.34791215  0.15634897]\n",
      " [ 1.23029068  1.20237985 -0.38732682 -0.30230275]\n",
      " [-1.04855297 -1.42001794 -1.70627019  1.9507754 ]\n",
      " [-0.50965218 -0.4380743  -1.25279536  0.77749036]\n",
      " [-1.61389785 -0.21274028 -0.89546656  0.3869025 ]\n",
      " [-0.51080514 -1.18063218 -0.02818223  0.42833187]\n",
      " [ 0.06651722  0.3024719  -0.63432209 -0.36274117]\n",
      " [-0.67246045 -0.35955316 -0.81314628 -1.7262826 ]\n",
      " [ 0.17742614 -0.40178094 -1.63019835  0.46278226]\n",
      " [-0.90729836  0.0519454   0.72909056  0.12898291]\n",
      " [ 1.13940068 -1.23482582  0.40234164 -0.68481009]\n",
      " [-0.87079715 -0.57884966 -0.31155253  0.05616534]\n",
      " [-1.16514984  0.90082649  0.46566244 -1.53624369]\n",
      " [ 1.48825219  1.89588918  1.17877957 -0.17992484]\n",
      " [-1.07075262  1.05445173 -0.40317695  1.22244507]\n",
      " [ 0.20827498  0.97663904  0.3563664   0.70657317]\n",
      " [ 0.01050002  1.78587049  0.12691209  0.40198936]\n",
      " [ 1.8831507  -1.34775906 -1.270485    0.96939671]\n",
      " [-1.17312341  1.94362119 -0.41361898 -0.74745481]\n",
      " [ 1.92294203  1.48051479  1.86755896  0.90604466]\n",
      " [-0.86122569  1.91006495 -0.26800337  0.8024564 ]\n",
      " [ 0.94725197 -0.15501009  0.61407937  0.92220667]\n",
      " [ 0.37642553 -1.09940079  0.29823817  1.3263859 ]\n",
      " [-0.69456786 -0.14963454 -0.43515355  1.84926373]\n",
      " [ 0.67229476  0.40746184 -0.76991607  0.53924919]\n",
      " [-0.67433266  0.03183056 -0.63584608  0.67643329]\n",
      " [ 0.57659082 -0.20829876  0.39600671 -1.09306151]\n",
      " [-1.49125759  0.4393917   0.1666735   0.63503144]\n",
      " [ 2.38314477  0.94447949 -0.91282223  1.11701629]\n",
      " [-1.31590741 -0.4615846  -0.06824161  1.71334272]\n",
      " [-0.74475482 -0.82643854 -0.09845252 -0.66347829]\n",
      " [ 1.12663592 -1.07993151 -1.14746865 -0.43782004]\n",
      " [-0.49803245  1.92953205  0.94942081  0.08755124]\n",
      " [-1.22543552  0.84436298 -1.00021535 -1.5447711 ]\n",
      " [ 1.18802979  0.31694261  0.92085882  0.31872765]\n",
      " [ 0.85683061 -0.65102559 -1.03424284  0.68159452]\n",
      " [-0.80340966 -0.68954978 -0.4555325   0.01747916]\n",
      " [-0.35399391 -1.37495129 -0.6436184  -2.22340315]\n",
      " [ 0.62523145 -1.60205766 -1.10438334  0.05216508]\n",
      " [-0.739563    1.5430146  -1.29285691  0.26705087]\n",
      " [-0.03928282 -1.1680935   0.52327666 -0.17154633]\n",
      " [ 0.77179055  0.82350415  2.16323595  1.33652795]\n",
      " [-0.36918184 -0.23937918  1.0996596   0.65526373]\n",
      " [ 0.64013153 -1.61695604 -0.02432612 -0.73803091]\n",
      " [ 0.2799246  -0.09815039  0.91017891  0.31721822]\n",
      " [ 0.78632796 -0.4664191  -0.94444626 -0.41004969]\n",
      " [-0.01702041  0.37915174  2.25930895 -0.04225715]\n",
      " [-0.955945   -0.34598178 -0.46359597  0.48148147]\n",
      " [-1.54079701  0.06326199  0.15650654  0.23218104]\n",
      " [-0.59731607 -0.23792173 -1.42406091 -0.49331988]\n",
      " [-0.54286148  0.41605005 -1.15618243  0.7811981 ]\n",
      " [ 1.49448454 -2.06998503  0.42625873  0.67690804]\n",
      " [-0.63743703 -0.39727181 -0.13288058 -0.29779088]\n",
      " [-0.30901297 -1.67600381  1.15233156  1.07961859]\n",
      " [-0.81336426 -1.46642433  0.52106488 -0.57578797]\n",
      " [ 0.14195316 -0.31932842  0.69153875  0.69474914]\n",
      " [-0.72559738 -1.38336396 -1.5829384   0.61037938]\n",
      " [-1.18885926 -0.50681635 -0.59631404 -0.0525673 ]\n",
      " [-1.93627981  0.1887786   0.52389102  0.08842209]\n",
      " [-0.31088617  0.09740017  0.39904635 -2.77259276]\n",
      " [ 1.95591231  0.39009332 -0.65240858 -0.39095338]\n",
      " [ 0.49374178 -0.11610394 -2.03068447  2.06449286]\n",
      " [-0.11054066  1.02017271 -0.69204985  1.53637705]\n",
      " [ 0.28634369  0.60884383 -1.04525337  1.21114529]\n",
      " [ 0.68981816  1.30184623 -0.62808756 -0.48102712]\n",
      " [ 2.3039167  -1.06001582 -0.1359497   1.13689136]\n",
      " [ 0.09772497  0.58295368 -0.39944903  0.37005589]\n",
      " [-1.30652685  1.65813068 -0.11816405 -0.6801782 ]\n",
      " [ 0.66638308 -0.46071979 -1.33425847 -1.34671751]\n",
      " [ 0.69377315 -0.15957344 -0.13370156  1.07774381]\n",
      " [-1.12682581 -0.73067775 -0.38487981  0.09435159]\n",
      " [-0.04217145 -0.28688719 -0.0616264  -0.10730528]\n",
      " [-0.71960439 -0.81299299  0.27451636 -0.89091508]\n",
      " [-1.15735526 -0.31229225 -0.15766702  2.2567235 ]\n",
      " [-0.70470028  0.94326072  0.74718833 -1.18894496]\n",
      " [ 0.77325298 -1.18388064 -2.65917224  0.60631952]\n",
      " [-1.75589058  0.45093446 -0.6840109   1.6595508 ]\n",
      " [ 1.0685094  -0.4533858  -0.68783761 -1.2140774 ]\n",
      " [-0.44092263 -0.2803555  -0.36469354  0.15670386]\n",
      " [ 0.5785215   0.34965446 -0.76414392 -1.43779147]\n",
      " [ 1.36453185 -0.68944918 -0.6522936  -0.52118931]\n",
      " [-1.84306955 -0.477974   -0.47965581  0.6203583 ]\n",
      " [ 0.69845715  0.00377089  0.93184837  0.33996498]\n",
      " [-0.01568211  0.16092817 -0.19065349 -0.39484951]\n",
      " [-0.26773354 -1.12801133  0.28044171 -0.99312361]\n",
      " [ 0.84163126 -0.24945858  0.04949498  0.49383678]\n",
      " [ 0.64331447 -1.57062341 -0.20690368  0.88017891]\n",
      " [-1.69810582  0.38728048 -2.25556423 -1.02250684]\n",
      " [ 0.03863055 -1.6567151  -0.98551074 -1.47183501]\n",
      " [ 1.64813493  0.16422776  0.56729028 -0.2226751 ]\n",
      " [-0.35343175 -1.61647419 -0.29183736 -0.76149221]\n",
      " [ 0.85792392  1.14110187  1.46657872  0.85255194]]\n",
      "[ 49.82290745   4.87728597  11.91487464  19.75307803  23.60700003\n",
      " -29.98278439  -3.18275233  81.61798896 -85.87627478  66.296663\n",
      " -70.00907918 -25.41289672 -40.341885   -50.80958638  11.69850317\n",
      " -26.06617118 -10.09289693 -16.81772822 -18.84474658 -37.62388016\n",
      "   6.90439112  95.27998841  14.08815923  37.63961941  61.23802841\n",
      "  -7.46245465  42.37309217  89.99454831  47.61800511  14.11477091\n",
      " -29.85246699 -19.3463247   27.69980617 -12.73102933   4.69818239\n",
      " -15.54546881  81.10896741 -42.73831876 -43.50103947 -13.81361736\n",
      "  55.72588193   3.73964726  35.17541383  -4.68693299 -40.02555297\n",
      " -54.23600024 -41.92953467  37.56912025 -40.71853261  43.95479823\n",
      " -15.74495171 -42.13327197   2.38253778   0.17626991  12.60675473\n",
      " -31.41170946 -29.41293009 -20.37016177   3.09183413 -40.10546979\n",
      " -26.63729755 -63.60114181 -66.77520312  -8.00243526 -62.13849458\n",
      " -41.68036152 -33.22843061  -3.04264873  53.41069146   6.15068883\n",
      "  32.59386934  26.6719395   58.61984841  10.99216958  21.92203341\n",
      "  29.88418279  -2.08694047   8.76446094 -48.058451   -10.66707421\n",
      " -42.5262156  -34.38791803  17.79006328 -24.60719319 -20.57401167\n",
      "   6.40418112 -18.61524357  23.80290271   4.40414106 -54.10114237\n",
      "  14.44189199   5.17752156 -44.0304289    8.72306551 -40.48486858\n",
      " -21.5649097  -55.81801122  39.38582014 -62.477272    56.57212904]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import model as im\n",
    "\n",
    "X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 100],\n",
    "    'max_depth': [2, 3]\n",
    "}\n",
    "\n",
    "oob_gridsearch = OOB_ParamGridSearch(n_jobs=1,\n",
    "                                     estimator=im.IterativeRFRegression,\n",
    "                                     param_grid=param_grid,\n",
    "                                     seed=123,\n",
    "                                     refit=True,\n",
    "                                     task=\"regression\",\n",
    "                                     metric=\"mse\")\n",
    "\n",
    "\n",
    "oob_gridsearch.fit(X_train=X, y_train=y)\n",
    "oob_gridsearch.cv_results\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing code for testing\n",
    "+ fit\n",
    "+ fit_score\n",
    "+ oob_score_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   OOB_Error_Score  max_depth  n_estimators\n",
      "0         0.033333          3           100\n",
      "1         0.040000          3            30\n",
      "2         0.046667          3            20\n",
      "3         0.053333          2            30\n",
      "4         0.060000          2            20\n",
      "5         0.060000          2           100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import datasets\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import model as im\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.load_iris(as_frame=True)\n",
    "\n",
    "# Separate out the data\n",
    "X = dataset['data']\n",
    "y = dataset['target']\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 100],\n",
    "    'max_depth': [2, 3]\n",
    "}\n",
    "\n",
    "def fit(X, y, param_grid):\n",
    "\n",
    "    params_iterable = list(ParameterGrid(param_grid))\n",
    "\n",
    "    parallel = joblib.Parallel(n_jobs=1)\n",
    "\n",
    "    output = parallel(\n",
    "              joblib.delayed(_fit_and_score)(deepcopy(\n",
    "                im.IterativeRFClassifier), X, y,parameters)\n",
    "            for parameters in params_iterable)\n",
    "\n",
    "\n",
    "    n_candidates = len(params_iterable)\n",
    "    a=np.array(output, dtype=np.float64)\n",
    "\n",
    "    best_index = np.argmin(a)\n",
    "    best_score_ = a[best_index]\n",
    "    best_param_ = params_iterable[best_index]\n",
    "\n",
    "    cv_results = pd.DataFrame(output, columns=['OOB_Error_Score'])\n",
    "    df_params = pd.DataFrame(params_iterable)\n",
    "    cv_results = pd.concat([cv_results, df_params], axis = 1)\n",
    "\n",
    "\n",
    "    cv_results = (cv_results.\n",
    "                  sort_values(['OOB_Error_Score'],ascending=True).\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "    return cv_results\n",
    "\n",
    "def _fit_and_score(estimator, X, y, parameters):\n",
    "\n",
    "\n",
    "    train_model = estimator(rseed=1, **parameters)\n",
    "    train_model.fit(X, y,  feature_weight=None)\n",
    "    oob_error = 1 - oob_score_accuracy(train_model, X, y)\n",
    "\n",
    "    return oob_error\n",
    "\n",
    "\n",
    "def oob_score_accuracy(rf, X, y):\n",
    "    from sklearn.ensemble._forest import _generate_unsampled_indices, _get_n_samples_bootstrap\n",
    "\n",
    "    X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "\n",
    "    n_samples = len(X)\n",
    "    n_classes = len(np.unique(y))\n",
    "    predictions = np.zeros((n_samples, n_classes))\n",
    "    for tree in getattr(rf, \"model\").estimators_:\n",
    "        n_samples_bootstrap = _get_n_samples_bootstrap(n_samples, n_samples)\n",
    "        unsampled_indices = _generate_unsampled_indices(tree.random_state, n_samples, n_samples_bootstrap)\n",
    "\n",
    "        tree_preds = tree.predict_proba(X[unsampled_indices, :])\n",
    "        predictions[unsampled_indices] += tree_preds\n",
    "\n",
    "    predicted_class_indexs = np.argmax(predictions, axis=1)\n",
    "    predicted_class = [getattr(rf, \"model\").classes_[i] for i in predicted_class_indexs]\n",
    "    \n",
    "    oob_score = np.mean(y == predicted_class)\n",
    "    \n",
    "    return oob_score\n",
    "\n",
    "oob_gridsearch = fit(X, y, param_grid)\n",
    "print(oob_gridsearch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232.4546273335677"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_features=4, n_informative=2, random_state=0, shuffle=False)\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(X, y)\n",
    "prediction = regr.predict(X)\n",
    "\n",
    "def get_evaluation_report(y_pred, y_true, task, metric):\n",
    "    \"\"\"\n",
    "    Get values for common evaluation metrics\n",
    "\n",
    "    :param y_pred: predicted values\n",
    "    :param y_true: true values\n",
    "    :param task: ML task to solve\n",
    "    :param metic: choose specificed metric to assess the performance\n",
    "\n",
    "    :return: dictionary with specificed metrics\n",
    "    \"\"\"\n",
    "   \n",
    "    if task == 'classification':\n",
    "        average = 'micro' if len(np.unique(y_true)) > 2 else 'binary'\n",
    "        eval_report_dict = {\n",
    "            'auroc': sklearn.metrics.roc_auc_score(y_true=y_true, y_pred=y_pred, average=average),\n",
    "            'aupr': sklearn.metrics.average_precision_score(y_true=y_true, y_pred=y_pred, average=average)\n",
    "        }\n",
    "        eval_report_dict = eval_report_dict[metric]\n",
    "    else:\n",
    "        eval_report_dict = {\n",
    "            'mse': sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred),\n",
    "            'rmse': sklearn.metrics.mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False),\n",
    "            'r2_score': sklearn.metrics.r2_score(y_true=y_true, y_pred=y_pred),\n",
    "        }\n",
    "        eval_report_dict = eval_report_dict[metric]\n",
    "        \n",
    "    return eval_report_dict\n",
    "\n",
    "\n",
    "get_evaluation_report(prediction, y, task=\"regression\",metric=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF_OOB_Dataset function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  3, 244, 258, 405, 406, 520, 553, 655, 717, 805]),)\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from  pathlib import Path\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import sys\n",
    "import dataset_model as dm\n",
    "\n",
    "class ExpressionDataset(ABC):\n",
    "    \"\"\" \n",
    "    The base dataset defining the API for datasets in this project\n",
    "    \"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Abstract initializer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    @abstractmethod\n",
    "    def from_config(class_object):\n",
    "        \"\"\"\n",
    "        A function to initialize a ExpressionDataset object\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_samples(self):\n",
    "        \"\"\"\n",
    "        Return the sample ids for all samples in the dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def get_features(self):\n",
    "        \"\"\"\n",
    "        Return the list of the ids of all the features in the dataset\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_labels(self):\n",
    "        \"\"\"\n",
    "        Process the y matrix for the given phenotype trait\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save_pipeline(self):\n",
    "         \"\"\"\n",
    "         Save the version of the pipline\n",
    "         \"\"\"\n",
    "         raise NotImplementedError\n",
    "     \n",
    "    @abstractmethod\n",
    "    def load_pipeline(self):\n",
    "         \"\"\"\n",
    "         Load the version of the pipline\n",
    "         \"\"\"\n",
    "         raise NotImplementedError\n",
    "    \n",
    "\n",
    "class TrainTestSplit(ExpressionDataset):\n",
    "    \"\"\"\n",
    "    A base train_test_split defining the API for train-test splitting\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def train_test_split(self,\n",
    "                         train_fraction,\n",
    "                         test_fraction,\n",
    "                         seed):\n",
    "        \"\"\"\n",
    "        Split the dataset into two portion, \n",
    "        as seen in scikit-learn's `train_test_split` function\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @abstractmethod\n",
    "    def shuffle_data(self, X, y, seed):\n",
    "        \"\"\"\n",
    "        Random shuffle of the samples in X and y\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class RF_OOB_Dataset(TrainTestSplit):\n",
    "    \"\"\"\n",
    "    A class containing logic used by all the types of gwas datasets for computing out of bag score\n",
    "    The RF_OOB_Dataset inheritance pattern from class ExpressionDataset and TrainTestSplit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 gwas_gen_dir,\n",
    "                 label_df_dir,\n",
    "                 env_df_dir):\n",
    "        \n",
    "        \"\"\"\n",
    "        An initializer for the class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.all_gen_df = pd.read_csv(gwas_gen_dir, sep=\",\")\n",
    "        self.all_gen_df = self.all_gen_df.drop(['FID','IID'], axis=1)\n",
    "        self.env_df = pd.read_csv(env_df_dir, sep=\"\\t\")\n",
    "        \n",
    "        self.all_gwas_df = pd.concat([self.all_gen_df, self.env_df], axis=1)\n",
    "        self.label_df = pd.read_csv(label_df_dir, sep=\"\\t\")\n",
    "    \n",
    "    @classmethod \n",
    "    def from_config(class_object,\n",
    "                    config_file,\n",
    "                    weight_tissue):\n",
    "        \"\"\"\n",
    "        A function to create a new object from paths to its data\n",
    "        \"\"\"\n",
    "        \n",
    "        data_dir = Path(config_file['dataset']['data_dir'])\n",
    "        gwas_df_dir = data_dir / weight_tissue / (\"predict_expression_\" + weight_tissue + \"_output.csv\")\n",
    "        \n",
    "        return class_object(gwas_df_dir, config_file['dataset']['phentoype_dir'], config_file['dataset']['env_dir'])\n",
    "    \n",
    "    def get_samples(self):\n",
    "        \"\"\"\n",
    "        Return the list of sample accessions for all samples currently available in the dataset\n",
    "        \"\"\"\n",
    "        return list(self.all_gwas_df.index)\n",
    "    \n",
    "    def get_features(self):\n",
    "        \"\"\"\n",
    "        Return the list of the ids of all the features in the currently available in the dataset \n",
    "        \"\"\"\n",
    "        return list(self.all_gwas_df.columns)\n",
    "    \n",
    "    def shuffle_data(self, X, y, seed):\n",
    "        \"\"\"\n",
    "        Random shuffle of the samples in X and y\n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        return X[idx], y[idx]\n",
    "    \n",
    "    def train_test_split(self, X, y, seed, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into train and test sets\n",
    "        \"\"\"\n",
    "        \n",
    "        X, y = self.shuffle_data(X, y, seed)\n",
    "        split_i = len(y) - int(len(y)// (1 / test_size))\n",
    "        X_train, X_test = X[:split_i], X[split_i:]\n",
    "        y_train , y_test = y[:split_i], y[split_i:]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def generate_labels(self, phen_trait):\n",
    "        \"\"\"\n",
    "        Random shuffle of the samples in X and y\n",
    "        \"\"\"\n",
    "        y_given_phen = self.label_df.loc[:, [phen_trait]]\n",
    "        \n",
    "        return y_given_phen\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_pipeline(pipeline_to_save, save_file_name):\n",
    "        \"\"\"\n",
    "        Save the version of the pipline\n",
    "        \"\"\"\n",
    "        joblib.dump(pipeline_to_save, save_file_name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_pipeline(pipline_file_path):\n",
    "        \"\"\"\n",
    "        Load the version of the pipline\n",
    "        \"\"\"\n",
    "        pipline_file_path  = joblib.load(filename=pipline_file_path)\n",
    "        return pipline_file_path\n",
    "\n",
    "try:\n",
    "    with open(\"/exeh_4/yuping/Epistasis_Interaction/02_Select_Parameter_Model/config.yaml\") as infile:\n",
    "        load_configure = yaml.safe_load(infile)\n",
    "except Exception:\n",
    "        sys.stderr.write(\"Please specify valid yaml file.\")\n",
    "        sys.exit(1)\n",
    "   \n",
    "\n",
    "RF_OOB_Dataset = dm.RF_OOB_Dataset.from_config(config_file=load_configure, \n",
    "                                               weight_tissue=\"Brain_Amygdala\")\n",
    "\n",
    "y_given_raw_df = RF_OOB_Dataset.generate_labels(\"BDS_Total\")\n",
    "\n",
    "\n",
    "X_raw_df = RF_OOB_Dataset.all_gwas_df.values if isinstance(RF_OOB_Dataset.all_gwas_df, pd.DataFrame) else RF_OOB_Dataset.all_gwas_df\n",
    "y_raw_df = y_given_raw_df.values if isinstance(y_given_raw_df, pd.DataFrame) else y_given_raw_df\n",
    "\n",
    "\n",
    "X_train_raw_df, X_test_raw_df, y_train_raw_df, y_test_raw_df = RF_OOB_Dataset.train_test_split(X_raw_df, \n",
    "                                                                                               y_raw_df, seed=1)\n",
    "\n",
    "X_train_df, X_test_df = pd.DataFrame(X_train_raw_df, columns=RF_OOB_Dataset.all_gwas_df.columns), pd.DataFrame(X_test_raw_df, columns=RF_OOB_Dataset.all_gwas_df.columns)\n",
    "y_train_df, y_test_df = pd.DataFrame(y_train_raw_df, columns=[\"BDS_Total\"]), pd.DataFrame(y_test_raw_df, columns=[\"BDS_Total\"])\n",
    "\n",
    "print(np.where(y_train_df.BDS_Total.isnull()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp_r",
   "language": "python",
   "name": "yp_r"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
