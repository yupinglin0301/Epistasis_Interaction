{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import _safe_indexing\n",
    "import numpy as np\n",
    "def _shuffle(y, groups, random_state):\n",
    "        \n",
    "\n",
    "    \"\"\"Return a shuffled copy of y eventually shuffle among same groups.\"\"\"\n",
    "    if groups is None:\n",
    "        indices = random_state.permutation(len(y))\n",
    "    else:\n",
    "        indices = np.arange(len(groups))\n",
    "        for group in np.unique(groups):\n",
    "            this_mask = groups == group\n",
    "            indices[this_mask] = random_state.permutation(indices[this_mask])\n",
    "    return _safe_indexing(y, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from select_parameter_utils import group_shuffle_split, OOB_Search, run_RIT, get_rit_counts\n",
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "import joblib\n",
    "\n",
    "\n",
    "def run_ITF(X, y, load_configure, rit_params, train_model, group_labels):\n",
    "\n",
    "   \n",
    "    X_train_raw_df, X_test_raw_df, y_train_tran_df, y_test_tran_df, train_index, test_index = group_shuffle_split(X, \n",
    "                                                                                                                  y, \n",
    "                                                                                                                  seed=load_configure['default_seed'],\n",
    "                                                                                                                  test_size=0.2,\n",
    "                                                                                                                  groups=group_labels)\n",
    "\n",
    "    oob_gridsearch = OOB_Search(n_jobs=1,\n",
    "                                estimator=train_model,\n",
    "                                param_grid={\"K\":[1]})\n",
    "\n",
    "    oob_gridsearch.fit(X_train=X_train_raw_df, y_train=y_train_tran_df)\n",
    "\n",
    "\n",
    "\n",
    "    all_rf_weights, cv_results= oob_gridsearch.extract_oob_result(oob_gridsearch.output_array, oob_gridsearch.params_iterable)\n",
    "    \n",
    "    \n",
    "    # Convert the bootstrap resampling proportion to the number\n",
    "    # of rows to resample from the training data\n",
    "    n_samples = ceil(rit_params['propn_n_samples']* X_train_raw_df.shape[0])\n",
    "\n",
    "    all_rit_bootstrap_output = {}\n",
    "    output = joblib.Parallel(n_jobs=3)(\n",
    "        joblib.delayed(run_RIT)(deepcopy(train_model), \n",
    "                                X_train_raw_df, \n",
    "                                y_train_tran_df.flatten(), \n",
    "                                X_test_raw_df, \n",
    "                                y_test_tran_df.flatten(), \n",
    "                                n_samples, \n",
    "                                all_rf_weights[0], \n",
    "                                **rit_params)\n",
    "        for b in range(0, rit_params['n_bootstrapped'])) \n",
    "    for i in (range(0,rit_params['n_bootstrapped'])):\n",
    "          all_rit_bootstrap_output['rf_bootstrap{}'.format(i)] = output[i]\n",
    "     \n",
    "    # Create a DataFrame from the data\n",
    "    feature_name = [f\"Feature{i}\" for i in range(1, 21)]\n",
    "    nub_feature = list(range(len(feature_name)))\n",
    "    feature_dict = {feature_name[i]: nub_feature[i] for i in range(len(feature_name))}\n",
    "    # get each bootstrapp sample's interaction term\n",
    "    rit_counts = joblib.Parallel(n_jobs=3)(\n",
    "            joblib.delayed(get_rit_counts)(b, \n",
    "                                           all_rit_bootstrap_output, \n",
    "                                           feature_dict)\n",
    "            for b in range(rit_params['n_bootstrapped']))   \n",
    "\n",
    "    all_rit_interactions = [item for sublist in rit_counts for item in sublist]\n",
    "    stability_score = {m: all_rit_interactions.count(m) / rit_params['n_bootstrapped'] for m in all_rit_interactions}\n",
    "    \n",
    "    return stability_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Feature3_Feature4': 0.3, 'Feature2_Feature3': 0.1, 'Feature15_Feature18': 0.1, 'Feature3_Feature18': 0.2, 'Feature3_Feature15': 0.1, 'Feature4_Feature15': 0.2}, {'Feature4_Feature15': 0.2, 'Feature3_Feature18': 0.3, 'Feature3_Feature4': 0.3, 'Feature4_Feature18': 0.1, 'Feature2_Feature3': 0.1}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "import model as im \n",
    "import yaml\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "X, y = make_regression(n_samples=60, n_features=20)\n",
    "    \n",
    "# Generate group labels\n",
    "group_labels = np.repeat(np.arange(3), 20)\n",
    "\n",
    "\n",
    "parameters = {'max_depth': 2, 'n_estimators': 2, 'oob_score': True}\n",
    "\n",
    "configure_file = \"/exeh_4/yuping/Epistasis_Interaction/02_Explore_interaction_term/model_configure/IRF_RF.yaml\"\n",
    "try:\n",
    "    with open(configure_file) as infile:\n",
    "        load_configure = yaml.safe_load(infile)\n",
    "except Exception:\n",
    "        sys.stderr.write(\"Please specify valid yaml file.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "rit_params = {\n",
    "    'n_intersection_tree' :  load_configure['model_params']['n_intersection_tree'][0],\n",
    "    'max_depth' : load_configure['model_params']['max_depth'][0],\n",
    "    'num_splits' : load_configure['model_params']['num_splits'][0],\n",
    "    'n_bootstrapped': load_configure['model_params']['n_bootstrapped'][0],\n",
    "    'propn_n_samples' : load_configure['model_params']['propn_n_samples'][0]\n",
    "}\n",
    "\n",
    "\n",
    "train_model = im.IterativeRFRegression(rseed=1, **parameters)\n",
    "\n",
    "\n",
    "\n",
    "permutation_scores = joblib.Parallel(n_jobs=2)(\n",
    "        joblib.delayed(run_ITF)(X,\n",
    "                                _shuffle(y, group_labels, np.random.RandomState(load_configure['default_seed'])),\n",
    "                                load_configure,\n",
    "                                rit_params,\n",
    "                                train_model,\n",
    "                                group_labels)\n",
    "        for _ in range(2)\n",
    ")\n",
    "\n",
    "print(permutation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 4, 'b': 5}, {'a': 7, 'b': 0}]\n",
      "   a  b\n",
      "0  4  5\n",
      "1  7  0\n",
      "[{'a': 4, 'b': 5}, {'a': 7, 'b': 0}]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_dicts(observed_data, perm_data):\n",
    "    # Create a set of keys present in dict1\n",
    "    keys_set = set(observed_data.keys())\n",
    "    \n",
    "    # Initialize a list to store modified dictionaries\n",
    "    modified_dicts = []\n",
    "    \n",
    "    # Iterate over each dictionary in other_dicts\n",
    "    for other_dict in perm_data:\n",
    "        # Create a copy of the dictionary\n",
    "        modified_dict = other_dict.copy()\n",
    "        \n",
    "        # Update the dictionary with keys from dict1\n",
    "        for key in list(modified_dict.keys()):  # Convert to list to avoid modifying dict during iteration\n",
    "            # If the key is not present in dict1, remove it from the dictionary\n",
    "            if key not in keys_set:\n",
    "                del modified_dict[key]\n",
    "        \n",
    "        # Add missing keys from dict1 with value 0\n",
    "        for key in keys_set:\n",
    "            if key not in modified_dict:\n",
    "                modified_dict[key] = 0\n",
    "        \n",
    "        # Append the modified dictionary to the list\n",
    "        modified_dicts.append(modified_dict)\n",
    "    \n",
    "    return modified_dicts\n",
    "\n",
    "# Example dictionaries\n",
    "Observed_data = {'a':4, 'b':5}\n",
    "Perm_data = [{'a': 4, 'b': 5, 'd': 6}, {'a': 7, 'c': 8}]  # Example list of dictionaries\n",
    "\n",
    "# Compare dictionaries\n",
    "modified_dicts = compare_dicts(Observed_data, Perm_data)\n",
    "print(modified_dicts)\n",
    "\n",
    "# Create DataFrame\n",
    "data = {key: [d.get(key, 0) for d in modified_dicts] for key in Observed_data.keys()}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "print(modified_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Pval\n",
      "Feature11_Feature18  0.333333\n",
      "Feature2_Feature12   0.333333\n",
      "{'Feature11_Feature18': [0, 0], 'Feature2_Feature12': [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "observed_data = {'Feature11_Feature18': 0.4, 'Feature2_Feature12': 0.3}\n",
    "# Compare dictionaries\n",
    "#modified_dicts = compare_dicts(observed_data, permutation_scores)\n",
    "# Create DataFrame\n",
    "data = {key: [d.get(key, 0) \n",
    "              for d in compare_dicts(observed_data, permutation_scores)] \n",
    "        for key in observed_data.keys()\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "feature_pval = []\n",
    "for i in df.columns.to_list():\n",
    "    a=df[i].to_list()\n",
    "    b=observed_data[i]\n",
    "    \n",
    "    # Perform element-wise comparison and count the number of values greater than or equal to b\n",
    "    count_greater_or_equal = sum(1 for x in a if x >= b)\n",
    "    \n",
    "    # Calculate the p-value\n",
    "    pvalue = (count_greater_or_equal + 1) / (len(a) + 1)\n",
    "    \n",
    "    feature_pval.append(pvalue)\n",
    "    \n",
    "\n",
    "feature_pval = pd.DataFrame(feature_pval, index=df.columns.to_list())\n",
    "# Assigning column names\n",
    "feature_pval.columns = ['Pval']\n",
    "\n",
    "\n",
    "print(feature_pval)\n",
    "\n",
    "print(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yp_IRF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
